{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import gym\n",
    "from MarkovDecisionProcess import MarkovDecisionProcess as MDP\n",
    "from Agent import Agent\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Policy:  [[0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0, 0, 1, 0]]\nState 0, policy(s): 2, value(s): 1.0\nState 1, policy(s): 2, value(s): 1.0\nState 2, policy(s): 1, value(s): 1.0\nState 3, policy(s): 1, value(s): 0.0\nState 4, policy(s): 1, value(s): 1.0\nState 5, policy(s): 1, value(s): 0.0\nState 6, policy(s): 1, value(s): 1.0\nState 7, policy(s): 1, value(s): 0.0\nState 8, policy(s): 2, value(s): 1.0\nState 9, policy(s): 2, value(s): 1.0\nState 10, policy(s): 1, value(s): 1.0\nState 11, policy(s): 1, value(s): 0.0\nState 12, policy(s): 2, value(s): 0.0\nState 13, policy(s): 2, value(s): 1.0\nState 14, policy(s): 2, value(s): 1.0\nState 15, policy(s): 2, value(s): 0.0\n[[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)\n",
    "agent = Agent(mdp)\n",
    "print(\"Policy: \", agent.policy)\n",
    "num_iters = 100\n",
    "agent.policy_iteration(num_iters)\n",
    "agent.print_agent_info()\n",
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n  (Right)\nS\u001b[41mF\u001b[0mFF\nFHFH\nFFFH\nHFFG\n  (Right)\nSF\u001b[41mF\u001b[0mF\nFHFH\nFFFH\nHFFG\n  (Down)\nSFFF\nFH\u001b[41mF\u001b[0mH\nFFFH\nHFFG\n  (Down)\nSFFF\nFHFH\nFF\u001b[41mF\u001b[0mH\nHFFG\n  (Down)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "observation = 0\n",
    "done = False\n",
    "env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from MarkovDecisionProcess import MarkovDecisionProcess as MDP\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()\n",
    "mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)\n",
    "print(\"Number of states \", mdp.num_states)\n",
    "print(\"Number of actions \", mdp.num_actions)\n",
    "sample_actions = {'LEFT':0, 'UP':3}\n",
    "sample_states = [0,11,15]\n",
    "for s in sample_states:\n",
    "    for a in sample_actions.keys():\n",
    "        print(f\"Transitions for state {s} and action {a} are\\n \", mdp.P[s][sample_actions[a]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}